{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAGGLE - Binary Classification code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import log_loss, roc_auc_score, recall_score, precision_score, average_precision_score, f1_score, classification_report, accuracy_score,confusion_matrix, silhouette_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.chdir(\"/Users/yj.noh/Documents/GitHub\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/yj.noh/Documents/GitHub/kaggle-Binary/train.csv\", encoding = \"utf-8\")\n",
    "test = pd.read_csv(\"/Users/yj.noh/Documents/GitHub/kaggle-Binary/test.csv\", encoding = \"utf-8\")\n",
    "\n",
    "print(train.shape) #101,763, 23\n",
    "print(test.shape) # 67,842"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA check \n",
    "print(train.isna().sum())\n",
    "print(test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category화 할수 있는 변수 존재? \n",
    "n = train.nunique(axis=0) \n",
    "print(\"No.of.unique values :\",  n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 체크\n",
    "duplicates = train[train.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome -> int \n",
    "train[\"defects\"] = train[\"defects\"].astype(int) \n",
    "\n",
    "print(train[\"defects\"].value_counts()) #0 : 78699, 1: 23064 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['defects', 'id'], axis=1)\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "y_train = train[['defects']].values.ravel()  # y_train을 1차원 배열로 변환\n",
    "\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = X_train.corr(method = 'pearson')\n",
    "fig, ax = plt.subplots(figsize=(18,18)) \n",
    "ax = sns.heatmap(cor, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns)\n",
    "print(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train_scaled, index = X_train.index, columns = X_train.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, index = X_test.index, columns = X_test.columns)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GLM - benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_1 = sm.GLM(y_train, X_train, family = sm.families.Binomial())\n",
    "glm1_fit = glm_1.fit()\n",
    "print(glm1_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-value 낮은 거 제외 \n",
    "excluded_variables = ['v(g)', 'ev(g)', 'v', 'd', 'e', 'b', 'total_Op', 'branchCount']\n",
    "X_train_v2 = X_train.drop(excluded_variables, axis=1)\n",
    "print(X_train_v2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_2 = sm.GLM(y_train, X_train_v2, family = sm.families.Binomial())\n",
    "glm2_fit = glm_2.fit()\n",
    "print(glm2_fit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    rfe = RFE(lr, step = 10) \n",
    "    fit = rfe.fit(X_train, y_train)\n",
    "    print(\"Features: {features}\".format(features=X_train.columns))\n",
    "    print(\"Num Features: {number_features}\".format(number_features=fit.n_features_))\n",
    "    print(\"Selected Features: {support}\".format(support=fit.support_))\n",
    "    print(\"Feature Ranking: {ranking}\".format(ranking=fit.ranking_))\n",
    "\n",
    "    selected_columns = [column for column, selected in zip(X_train.columns, fit.support_) if selected]\n",
    "    print(\"Selected columns: {selected}\".format(selected = selected_columns))\n",
    "\n",
    "X_train_v3 = X_train[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_to_test = [\n",
    "    {0: 0.1, 1: 0.9},\n",
    "    {0: 0.15, 1: 0.85},  \n",
    "    {0: 0.2, 1: 0.8}, \n",
    "    {0: 0.25, 1: 0.75}, \n",
    "    {0: 0.3, 1: 0.7},\n",
    "    {0: 0.35, 1: 0.65},\n",
    "    {0: 0.4, 1: 0.6},\n",
    "    {0: 0.45, 1: 0.55},\n",
    "    {0: 0.5, 1: 0.5},\n",
    "    {0: 0.55, 1: 0.45},\n",
    "    {0: 0.6, 1: 0.4}\n",
    "]\n",
    "\n",
    "best_class_weights = []\n",
    "best_accuracies = []\n",
    "\n",
    "for data in [X_train, X_train_v2, X_train_v3]:\n",
    "    best_class_weight = None\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for class_weight in class_weights_to_test:\n",
    "        model = LogisticRegression(class_weight = class_weight)\n",
    "        accuracy = cross_val_score(model, data, y_train, cv=10, scoring = make_scorer(accuracy_score)).mean()\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_class_weight = class_weight\n",
    "    \n",
    "    best_class_weights.append(best_class_weight)\n",
    "    best_accuracies.append(best_accuracy)\n",
    "\n",
    "# 결과 출력\n",
    "for i, (best_class_weight, accuracy) in enumerate(zip(best_class_weights, best_accuracies), start=1):\n",
    "    print(f\"Data (X_train_v{i}) - Best Class Weight: {best_class_weight}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.2),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 64),\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    \n",
    "    accuracy = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy', n_jobs=-1).mean()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_lgbm_model = lgb.LGBMClassifier(**best_params)\n",
    "\n",
    "accuracy = cross_val_score(best_lgbm_model, X_train, y_train, cv=10, scoring='accuracy', n_jobs=-1).mean()\n",
    "\n",
    "\n",
    "print(f'Best LightGBM Model Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "best_lgbm_model.fit(X_train, y_train)\n",
    "y_pred = best_lgbm_model.predict(X_test)\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame({'ID': X_test['id'], 'defects': y_pred})\n",
    "\n",
    "output_df.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
